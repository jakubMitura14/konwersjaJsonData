{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "from subprocess import Popen\n",
    "import subprocess\n",
    "import SimpleITK as sitk\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import functools\n",
    "from functools import partial\n",
    "import sys\n",
    "import os.path\n",
    "from os import path as pathOs\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import shutil\n",
    "from os.path import basename, dirname, exists, isdir, join, split\n",
    "from pathlib import Path\n",
    "import fileinput\n",
    "import re\n",
    "import subprocess\n",
    "from toolz.itertoolz import groupby\n",
    "import seaborn as sns\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import SimpleITK as sitk\n",
    "import mdai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pydicom\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import functools\n",
    "from functools import partial\n",
    "import mdai\n",
    "import math\n",
    "import time\n",
    "import itertools\n",
    "from pydicom.fileset import FileSet\n",
    "from os import path as pathOs\n",
    "from pathlib import Path\n",
    "import toolz\n",
    "from toolz.curried import pipe, map, filter, get\n",
    "from toolz import curry\n",
    "from os.path import basename, dirname, exists, isdir, join, split\n",
    "import nnunetv2\n",
    "import tempfile\n",
    "import shutil\n",
    "import re\n",
    "from toolz.itertoolz import groupby\n",
    "from toolz import curry\n",
    "# import multiprocess\n",
    "# p = multiprocess.Pool(os.cpu_count())\n",
    "import multiprocessing as mp\n",
    "import json\n",
    "import os\n",
    "from subprocess import Popen\n",
    "import subprocess\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from scipy import ndimage\n",
    "import torch\n",
    "import xformers.components.attention.attention_patterns as AP\n",
    "from xformers.components.attention.core import scaled_dot_product_attention\n",
    "from xformers.components.attention._sputnik_sparse import SparseCS\n",
    "import xformers.components.attention.attention_patterns as AP\n",
    "%matplotlib inline\n",
    "from xformers.utils import (\n",
    "    generate_matching_config,\n",
    "    get_registry_decorator,\n",
    "    import_all_modules,\n",
    ")\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, Set, Union\n",
    "\n",
    "from xformers.utils import (\n",
    "    generate_matching_config,\n",
    "    get_registry_decorator,\n",
    "    import_all_modules,\n",
    ")\n",
    "\n",
    "from xformers.components.feedforward.base import Feedforward, FeedforwardConfig  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"window partition operation based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "     Args:\n",
    "        x: input tensor.\n",
    "        window_size: local window size.\n",
    "    \"\"\"\n",
    "    x_shape = x.size()\n",
    "    if len(x_shape) == 5:\n",
    "        b, d, h, w, c = x_shape\n",
    "        x = x.view(\n",
    "            b,\n",
    "            d // window_size[0],\n",
    "            window_size[0],\n",
    "            h // window_size[1],\n",
    "            window_size[1],\n",
    "            w // window_size[2],\n",
    "            window_size[2],\n",
    "            c,\n",
    "        )\n",
    "        windows = (\n",
    "            x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, window_size[0] * window_size[1] * window_size[2], c)\n",
    "        )\n",
    "    elif len(x_shape) == 4:\n",
    "        b, h, w, c = x.shape\n",
    "        x = x.view(b, h // window_size[0], window_size[0], w // window_size[1], window_size[1], c)\n",
    "        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0] * window_size[1], c)\n",
    "    return windows\n",
    "\n",
    "def compute_mask(dims, window_size, shift_size, device):\n",
    "    \"\"\"Computing region masks based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "     Args:\n",
    "        dims: dimension values.\n",
    "        window_size: local window size.\n",
    "        shift_size: shift size.\n",
    "        device: device.\n",
    "    \"\"\"\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    if len(dims) == 3:\n",
    "        d, h, w = dims\n",
    "        img_mask = torch.zeros((1, d, h, w, 1), device=device)\n",
    "        for d in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n",
    "            for h in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n",
    "                for w in slice(-window_size[2]), slice(-window_size[2], -shift_size[2]), slice(-shift_size[2], None):\n",
    "                    img_mask[:, d, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "    elif len(dims) == 2:\n",
    "        h, w = dims\n",
    "        img_mask = torch.zeros((1, h, w, 1), device=device)\n",
    "        for h in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n",
    "            for w in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "    mask_windows = window_partition(img_mask, window_size)\n",
    "    mask_windows = mask_windows.squeeze(-1)\n",
    "    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "    return attn_mask\n",
    "\n",
    "dims=(8,8)\n",
    "window_size=(4,4)\n",
    "shift_size=(1,1)\n",
    "device='cpu'\n",
    "res=compute_mask(dims, window_size, shift_size, device)\n",
    "res.shape\n",
    "sns.heatmap(res[2,:,:].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEEDFORWARD_REGISTRY: Dict[str, Any] = {}\n",
    "FEEDFORWARD_CLASS_NAMES: Set[str] = set()\n",
    "\n",
    "\n",
    "def build_feedforward(config: Union[Dict[str, Any], FeedforwardConfig]):\n",
    "    \"\"\"Builds a feedforward from a config.\n",
    "\n",
    "    This assumes a 'name' key in the config which is used to determine what\n",
    "    attention class to instantiate. For instance, a config `{\"name\": \"my_feedforward\",\n",
    "    \"foo\": \"bar\"}` will find a class that was registered as \"my_feedforward\"\n",
    "    (see :func:`register_feedforward`) and call .from_config on it.\"\"\"\n",
    "\n",
    "    if not isinstance(config, FeedforwardConfig):\n",
    "        config_instance = generate_matching_config(\n",
    "            config, FEEDFORWARD_REGISTRY[config[\"name\"]].config\n",
    "        )\n",
    "    else:\n",
    "        config_instance = config\n",
    "\n",
    "    return FEEDFORWARD_REGISTRY[config_instance.name].constructor.from_config(\n",
    "        config_instance\n",
    "    )\n",
    "FEEDFORWARD_CLASS_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic nd cases\n",
    "def _generate_nd_grid(*sizes):\n",
    "    coords = [torch.arange(s) for s in sizes]\n",
    "    return torch.meshgrid(*coords)\n",
    "\n",
    "\n",
    "def swin_attention_pattern(H, W, window_size, shift_size=0):\n",
    "    assert H % window_size == 0\n",
    "    assert W % window_size == 0\n",
    "    assert 0 <= shift_size < window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "    # input grid\n",
    "    i, j = _generate_nd_grid(H, W)\n",
    "    i, j = i + 0.5, j + 0.5\n",
    "\n",
    "    # anchors grid\n",
    "    # if shift is present, add extra element to the grid\n",
    "    # to account for the uneven partitioning\n",
    "    extra = int(shift_size % window_size != 0)\n",
    "    grid_h = H // window_size + extra\n",
    "    grid_w = W // window_size + extra\n",
    "\n",
    "    ii, jj = _generate_nd_grid(grid_h, grid_w)\n",
    "    # convert shift to be compatible with the paper representation\n",
    "    s = (-shift_size) % window_size\n",
    "    offset = window_size / 2 - s\n",
    "    ii = ii * window_size + offset\n",
    "    jj = jj * window_size + offset\n",
    "\n",
    "    input_coords = torch.stack([i.flatten(), j.flatten()], 1).float()\n",
    "    anchors_coords = torch.stack([ii.flatten(), jj.flatten()], 1).float()\n",
    "\n",
    "    anchor_id = torch.cdist(input_coords, anchors_coords, p=2).argmin(1)\n",
    "    mask = anchor_id[:, None] == anchor_id[None, :]\n",
    "    return mask\n",
    "\n",
    "aa=swin_attention_pattern(8,8, 4, 2)\n",
    "plt.imshow(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def swin_attention_pattern_3D(H, W,D, window_size, shift_size=0):\n",
    "    assert H % window_size == 0\n",
    "    assert W % window_size == 0\n",
    "    assert D % window_size == 0\n",
    "    assert 0 <= shift_size < window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "    # input grid\n",
    "    i, j ,k = _generate_nd_grid(H, W,D)\n",
    "    i, j ,k = i + 0.5, j + 0.5, k + 0.5\n",
    "\n",
    "    # anchors grid\n",
    "    # if shift is present, add extra element to the grid\n",
    "    # to account for the uneven partitioning\n",
    "    extra = int(shift_size % window_size != 0)\n",
    "    grid_h = H // window_size + extra\n",
    "    grid_w = W // window_size + extra\n",
    "    grid_d = D // window_size + extra\n",
    "\n",
    "    ii, jj ,kk= _generate_nd_grid(grid_h, grid_w,grid_d)\n",
    "    # convert shift to be compatible with the paper representation\n",
    "    s = (-shift_size) % window_size\n",
    "    offset = window_size / 2 - s\n",
    "    ii = ii * window_size + offset\n",
    "    jj = jj * window_size + offset\n",
    "    kk = kk * window_size + offset\n",
    "\n",
    "\n",
    "    input_coords = torch.stack([i.flatten(), j.flatten(),k.flatten()], 1).float()\n",
    "    anchors_coords = torch.stack([ii.flatten(), jj.flatten(),jj.flatten()], 1).float()\n",
    "\n",
    "    anchor_id = torch.cdist(input_coords, anchors_coords, p=2).argmin(1)\n",
    "    mask = anchor_id[:, None] == anchor_id[None, :]\n",
    "    return mask\n",
    "aa=swin_attention_pattern_3D(8,8,8, 4, 2)\n",
    "plt.imshow(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xformers.factory import xFormer, xFormerConfig\n",
    "from xformers.helpers.hierarchical_configs import (\n",
    "    BasicLayerConfig,\n",
    "    get_hierarchical_configuration,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_nd_grid(*sizes):\n",
    "    coords = [torch.arange(s) for s in sizes]\n",
    "    return torch.meshgrid(*coords)\n",
    "\n",
    "\n",
    "def local_nd_distance(*sizes, p=2.0, weights=None):\n",
    "    if weights is None:\n",
    "        weights = (1,) * len(sizes)\n",
    "    assert len(sizes) == len(weights)\n",
    "    grid = _generate_nd_grid(*sizes)\n",
    "    grid = [i.flatten() * w for i, w in zip(grid, weights)]\n",
    "    grid = torch.stack(grid, dim=1).float()\n",
    "    d = torch.cdist(grid, grid, p=p)\n",
    "    return d\n",
    "\n",
    "def local_nd_pattern(*sizes, distance, p=2.0):\n",
    "    d = local_nd_distance(*sizes, p=p)\n",
    "    # print(d)\n",
    "    return d < distance\n",
    "\n",
    "\n",
    "pat=local_nd_pattern(4,4,4,distance=1.5)\n",
    "# plt.imshow(pat)\n",
    "# d=local_nd_distance(6,6,6)\n",
    "# plt.imshow(d)\n",
    "\n",
    "attn_mask = SparseCS(pat, torch.device(\"cpu\"))\n",
    "attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # csr_matrix = SparseCSRTensor._wrap(\n",
    "        #     _shape, values, row_indices, row_offsets, column_indices, _transp_info\n",
    "        # )\n",
    "# SparseCSRTensor.from_dense(output)\n",
    "from xformers.ops import masked_matmul\n",
    "from xformers.sparse import BlockSparseTensor, SparseCSRTensor\n",
    "from xformers.ops import masked_matmul\n",
    "from xformers.sparse import BlockSparseTensor, SparseCSRTensor\n",
    "from xformers.components.attention._sputnik_sparse import _dense_to_sparse\n",
    "\n",
    "# SparseCSRTensor.from_dense(torch.ones(64,64))#+attn_mask\n",
    "_dense_to_sparse(torch.ones(64,64),'cpu')\n",
    "# attn_mask\n",
    " cls, shape, values, row_indices, row_offsets, column_indices, _transp_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xformers.sparse import SparseCSRTensor\n",
    "\n",
    "def _generate_nd_grid(*sizes):\n",
    "    coords = [torch.arange(s) for s in sizes]\n",
    "    return torch.meshgrid(*coords)\n",
    "\n",
    "\n",
    "def local_nd_distance_parts(*sizes, p=2.0, weights=None,start=0,end=10):\n",
    "    if weights is None:\n",
    "        weights = (1,) * len(sizes)\n",
    "    assert len(sizes) == len(weights)\n",
    "    grid = _generate_nd_grid(*sizes)\n",
    "    grid = [i.flatten() * w for i, w in zip(grid, weights)]\n",
    "    grid = torch.stack(grid, dim=1).float()\n",
    "    print(f\"kkkk {grid.shape}\")\n",
    "    d = torch.cdist(grid, grid[start:end,:], p=p)\n",
    "    return d\n",
    "\n",
    "def local_nd_pattern(*sizes, distance, p=2.0,start=0,end=10):\n",
    "    d = local_nd_distance_parts(*sizes, p=p\n",
    "                                ,start=start\n",
    "                                ,end=end)\n",
    "    # print(d)\n",
    "    return d < distance\n",
    "part_1= SparseCS(local_nd_pattern(4,4,4,distance=1.5,start=0,end=32), torch.device(\"cpu\"))\n",
    "part_2= SparseCS(local_nd_pattern(4,4,4,distance=1.5,start=8,end=32), torch.device(\"cpu\"))\n",
    "\n",
    "\n",
    "fused=SparseCS.wrap(shape=(64,64)\n",
    "              ,values= torch.cat([part_1.values,part_2.values],dim=-1)\n",
    "              ,row_indices= torch.cat([part_1.row_indices,part_2.row_indices],dim=-1)\n",
    "              ,row_offsets= torch.cat([part_2.row_offsets],dim=-1)#+part_1.row_offsets[-1]\n",
    "              ,column_indices= torch.cat([part_1.column_indices,part_2.column_indices],dim=-1)\n",
    "              ,_transp_info=(\n",
    "                  torch.cat([part_1._transp_info[0],part_2._transp_info[0]],dim=-1)\n",
    "                  ,torch.cat([part_1._transp_info[1],part_2._transp_info[1]],dim=-1)\n",
    "                  ,torch.cat([part_1._transp_info[2],part_2._transp_info[2]],dim=-1)\n",
    "                  ,torch.cat([part_1._transp_info[3],part_2._transp_info[3]],dim=-1)\n",
    "                            )    )\n",
    "# plt.imshow(pat)\n",
    "# d=local_nd_distance(6,6,6)\n",
    "# plt.imshow(d)\n",
    "\n",
    "#torch.Size([1, 24, 24, 48, 40]) a torch.Size([1, 48, 12, 24, 20]) b  torch.Size([1, 96, 6, 12, 10])  \n",
    "\n",
    "# attn_mask = SparseCS(pat, torch.device(\"cpu\"))\n",
    "print(f\"orig {attn_mask.row_offsets}  fused {fused.row_offsets} orig {attn_mask.row_offsets.shape}  fused {fused.row_offsets.shape}  \")\n",
    "torch.equal(attn_mask.row_offsets,fused.row_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "np.arange(3*3*3).reshape((3,3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import scipy\n",
    "\n",
    "# def _generate_nd_grid(*sizes):\n",
    "#     coords = [np.arange(s) for s in sizes]\n",
    "#     return np.meshgrid(*coords)\n",
    "\n",
    "# def local_nd_distance(*sizes, p=2.0, weights=None):\n",
    "#     if weights is None:\n",
    "#         weights = (1,) * len(sizes)\n",
    "#     assert len(sizes) == len(weights)\n",
    "#     grid = _generate_nd_grid(*sizes)\n",
    "#     grid = [i.flatten() * w for i, w in zip(grid, weights)]\n",
    "#     grid = np.stack(grid, axis=1)#.astype(float)\n",
    "#     print(grid.shape)\n",
    "#     d = scipy.spatial.distance.cdist(grid, grid, 'euclidean')\n",
    "#     print(d.shape)\n",
    "#     return d\n",
    "\n",
    "# def local_nd_pattern(*sizes, distance, p=2.0):\n",
    "#     d = local_nd_distance(*sizes, p=p)\n",
    "#     # print(d)\n",
    "#     return d < distance\n",
    "\n",
    "# # patB=local_nd_pattern(24,96,80,distance=1.5)\n",
    "# patB=local_nd_pattern(12,48,40,distance=1.5)\n",
    "# plt.imshow(patB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1\n",
    "img_size=(24,96,80)\n",
    "# img_size=(16,16,16)\n",
    "patch_size=(2,2,2)\n",
    "i_layer=1\n",
    "embed_dim=7\n",
    "\n",
    "def get_image_size():\n",
    "    p= patch_size\n",
    "    res=[batch_size,embed_dim,img_size[0]/p[0],img_size[1]/p[1],img_size[2]/p[2]]\n",
    "    for i in range(i_layer):\n",
    "        res[1]=res[1]*2\n",
    "        res[2]=int(res[2]//2)\n",
    "        res[3]=int(res[3]//2)\n",
    "        res[4]=int(res[4]//2)\n",
    "    return res\n",
    "\n",
    "get_image_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes=(16,16,16)\n",
    "grid = _generate_nd_grid(*sizes)\n",
    "weights = (1,) * len(sizes)\n",
    "grid = [i.flatten() * w for i, w in zip(grid, weights)]\n",
    "grid = np.stack(grid, axis=1)#.astype(float)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes=(3,3,3)\n",
    "grid = _generate_nd_grid(*sizes)\n",
    "grid = [i.flatten() * w for i, w in zip(grid, weights)]\n",
    "grid = torch.stack(grid, dim=1).float()\n",
    "res=einops.rearrange(grid,'(x y z) p->x y z p',x=3,y=3,z=3)\n",
    "# grid.shape $ torch.Size([27, 3])\n",
    "res[0,2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xformers.factory.model_factory import xFormer, xFormerConfig\n",
    "import torch\n",
    "\n",
    "EMB = 384\n",
    "SEQ = 1024\n",
    "BATCH = 16\n",
    "VOCAB = 64\n",
    "\n",
    "my_config = [\n",
    "    # A list of the encoder or decoder blocks which constitute the Transformer.\n",
    "    # Note that a sequence of different encoder blocks can be used, same for decoders\n",
    "    {\n",
    "        \"reversible\": False,  # Optionally make these layers reversible, to save memory\n",
    "        \"block_type\": \"encoder\",\n",
    "        \"num_layers\": 1,  # Optional, this means that this config will repeat N times\n",
    "        \"dim_model\": EMB,\n",
    "        \"residual_norm_style\": \"pre\",  # Optional, pre/post\n",
    "        \"position_encoding_config\": {\n",
    "            \"name\": \"vocab\",  # whatever position encodinhg makes sense\n",
    "            \"seq_len\": 1024,\n",
    "            \"vocab_size\": VOCAB,\n",
    "        },\n",
    "        \"multi_head_config\": {\n",
    "            \"num_heads\": 4,\n",
    "            \"residual_dropout\": 0,\n",
    "            \"attention\": {\n",
    "                \"name\": \"linformer\",  # whatever attention mechanism\n",
    "                \"dropout\": 0,\n",
    "                \"causal\": False,\n",
    "                \"seq_len\": SEQ,\n",
    "            },\n",
    "        },\n",
    "        \"feedforward_config\": {\n",
    "            \"name\": \"MLP\",\n",
    "            \"dropout\": 0,\n",
    "            \"activation\": \"relu\",\n",
    "            \"hidden_layer_multiplier\": 4,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"reversible\": False,  # Optionally make these layers reversible, to save memory\n",
    "        \"block_type\": \"decoder\",\n",
    "        \"num_layers\": 3,  # Optional, this means that this config will repeat N times\n",
    "        \"dim_model\": EMB,\n",
    "        \"residual_norm_style\": \"pre\",  # Optional, pre/post\n",
    "        \"position_encoding_config\": {\n",
    "            \"name\": \"vocab\",  # whatever position encodinhg makes sense\n",
    "            \"seq_len\": SEQ,\n",
    "            \"vocab_size\": VOCAB,\n",
    "        },\n",
    "        \"multi_head_config_masked\": {\n",
    "            \"num_heads\": 4,\n",
    "            \"residual_dropout\": 0,\n",
    "            \"attention\": {\n",
    "                \"name\": \"nystrom\",  # whatever attention mechanism\n",
    "                \"dropout\": 0,\n",
    "                \"causal\": True,\n",
    "                \"seq_len\": SEQ,\n",
    "            },\n",
    "        },\n",
    "        \"multi_head_config_cross\": {\n",
    "            \"num_heads\": 4,\n",
    "            \"residual_dropout\": 0,\n",
    "            \"attention\": {\n",
    "                \"name\": \"favor\",  # whatever attention mechanism\n",
    "                \"dropout\": 0,\n",
    "                \"causal\": True,\n",
    "                \"seq_len\": SEQ,\n",
    "            },\n",
    "        },\n",
    "        \"feedforward_config\": {\n",
    "            \"name\": \"MLP\",\n",
    "            \"dropout\": 0,\n",
    "            \"activation\": \"relu\",\n",
    "            \"hidden_layer_multiplier\": 4,\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# This part of xFormers is entirely type checked and needs a config object,\n",
    "# could be changed in the future\n",
    "config = xFormerConfig(my_config)\n",
    "model = xFormer.from_config(config)\n",
    "\n",
    "#  Test out with dummy inputs\n",
    "x = (torch.rand((BATCH, SEQ)) * VOCAB).abs().to(torch.int)\n",
    "y = model(src=x, tgt=x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"x {x.shape} y {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xformers.factory import xFormerEncoderBlock, xFormerEncoderConfig\n",
    "import torch\n",
    "\n",
    "BATCH = 8\n",
    "SEQ = 1024\n",
    "EMB = 384\n",
    "VOCAB = 64\n",
    "\n",
    "encoder_config = {\n",
    "    \"dim_model\": EMB,\n",
    "    \"residual_norm_style\": \"pre\",  # Optional, pre/post\n",
    "    \"position_encoding_config\": {\n",
    "        \"name\": \"vocab\",  # whatever position encodinhg makes sense\n",
    "        \"seq_len\": SEQ,\n",
    "        \"vocab_size\": VOCAB,\n",
    "    },\n",
    "    \"multi_head_config\": {\n",
    "        \"num_heads\": 4,\n",
    "        \"residual_dropout\": 0,\n",
    "        \"attention\": {\n",
    "            \"name\": \"scaled_dot_product\",#linformer\",  # whatever attention mechanism\n",
    "            \"dropout\": 0,\n",
    "            \"seq_len\": SEQ,\n",
    "        },\n",
    "    },\n",
    "    \"feedforward_config\": {\n",
    "        \"name\": \"MLP\",\n",
    "        \"dropout\": 0,\n",
    "        \"activation\": \"relu\",\n",
    "        \"hidden_layer_multiplier\": 4,\n",
    "    },\n",
    "}\n",
    "\n",
    "# \"constructing\" the config will lead to a lot of type checking,\n",
    "# which could catch some errors early on\n",
    "config = xFormerEncoderConfig(**encoder_config)\n",
    "\n",
    "encoder = xFormerEncoderBlock(config)\n",
    "\n",
    "#  Test out with dummy inputs\n",
    "x = (torch.rand((BATCH, SEQ)) * VOCAB).abs().to(torch.int)\n",
    "y = encoder(x, x, x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = torch.nn.TransformerEncoderLayer(d_model=512, nhead=32)\n",
    "src = torch.rand(10, 32, 512)\n",
    "out = encoder_layer(src)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay=0.1\n",
    "betas=(0.9, 0.95)\n",
    "n_embd=512\n",
    "block_size=128\n",
    "n_layer=1\n",
    "n_head=8\n",
    "resid_pdrop=0.1\n",
    "attn_pdrop=0.1\n",
    "mlp_pdrop=0.1\n",
    "attention=\"scaled_dot_product\"\n",
    "hidden_layer_multiplier=4\n",
    "warmup_tokens=20\n",
    "final_tokens=1000\n",
    "vocab_size = 64\n",
    "\n",
    "xformer_config = [\n",
    "    {\n",
    "        \"reversible\": False,  # Turn on to test the effect of using reversible layers\n",
    "        \"block_type\": \"encoder\",\n",
    "        \"num_layers\": n_layer,\n",
    "        \"dim_model\": n_embd,\n",
    "        \"residual_norm_style\": \"post\",\n",
    "        \"position_encoding_config\": {\n",
    "            \"name\": \"vocab\",\n",
    "            \"seq_len\": block_size,\n",
    "            \"vocab_size\": vocab_size,\n",
    "        },\n",
    "        \"multi_head_config\": {\n",
    "            \"num_heads\": n_head,\n",
    "            \"residual_dropout\": resid_pdrop,\n",
    "            \"use_rotary_embeddings\": True,\n",
    "            \"attention\": {\n",
    "                \"name\": attention,\n",
    "                \"dropout\": attn_pdrop,\n",
    "                \"causal\": True,\n",
    "                \"seq_len\": block_size,\n",
    "                \"num_rules\": n_head,\n",
    "            },\n",
    "        },\n",
    "        \"feedforward_config\": {\n",
    "            \"name\": \"FusedMLP\",  # Use MLP if Triton is not available\n",
    "            \"dropout\": mlp_pdrop,\n",
    "            \"activation\": \"gelu\",\n",
    "            \"hidden_layer_multiplier\": hidden_layer_multiplier,\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "config = xFormerConfig(xformer_config)\n",
    "config.weight_init = \"small\"\n",
    "model = xFormer.from_config(config)\n",
    "model(torch.ones(10,vocab_size,n_embd).int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "import torch.nn as nn\n",
    "from monai.networks.layers import DropPath, trunc_normal_\n",
    "\n",
    "class Relative_position_embedding_3d(nn.Module):\n",
    "    \"\"\"\n",
    "    Window based multi-head self attention module with relative position bias based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        curr_img_size: Sequence[int],\n",
    "    ) -> None:\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.curr_img_size = (int(curr_img_size[0]),int(curr_img_size[1]),int(curr_img_size[2]))\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "        mesh_args = torch.meshgrid.__kwdefaults__\n",
    "        \n",
    "        if len(self.curr_img_size) == 3:\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros(\n",
    "                    (2 * self.curr_img_size[0] - 1) * (2 * self.curr_img_size[1] - 1) * (2 * self.curr_img_size[2] - 1),\n",
    "                    num_heads,\n",
    "                )\n",
    "            )\n",
    "            coords_d = torch.arange(self.curr_img_size[0])\n",
    "            coords_h = torch.arange(self.curr_img_size[1])\n",
    "            coords_w = torch.arange(self.curr_img_size[2])\n",
    "            if mesh_args is not None:\n",
    "                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w, indexing=\"ij\"))\n",
    "            else:\n",
    "                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w))\n",
    "            coords_flatten = torch.flatten(coords, 1)\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "            relative_coords[:, :, 0] += self.curr_img_size[0] - 1\n",
    "            relative_coords[:, :, 1] += self.curr_img_size[1] - 1\n",
    "            relative_coords[:, :, 2] += self.curr_img_size[2] - 1\n",
    "            relative_coords[:, :, 0] *= (2 * self.curr_img_size[1] - 1) * (2 * self.curr_img_size[2] - 1)\n",
    "            relative_coords[:, :, 1] *= 2 * self.curr_img_size[2] - 1\n",
    "        elif len(self.curr_img_size) == 2:\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros((2 * curr_img_size[0] - 1) * (2 * curr_img_size[1] - 1), num_heads)\n",
    "            )\n",
    "            coords_h = torch.arange(self.curr_img_size[0])\n",
    "            coords_w = torch.arange(self.curr_img_size[1])\n",
    "            if mesh_args is not None:\n",
    "                coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing=\"ij\"))\n",
    "            else:\n",
    "                coords = torch.stack(torch.meshgrid(coords_h, coords_w))\n",
    "            coords_flatten = torch.flatten(coords, 1)\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "            relative_coords[:, :, 0] += self.curr_img_size[0] - 1\n",
    "            relative_coords[:, :, 1] += self.curr_img_size[1] - 1\n",
    "            relative_coords[:, :, 0] *= 2 * self.curr_img_size[1] - 1\n",
    "\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "        trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "        print(f\"rrrrrrrr relative_position_bias_table \\n{self.relative_position_bias_table}\")\n",
    "        sns.heatmap(self.relative_position_bias_table.detach().cpu().numpy())\n",
    "\n",
    "    def forward(self, attn,n):\n",
    "        relative_position_bias = self.relative_position_bias_table[\n",
    "            self.relative_position_index.clone()[:n, :n].reshape(-1)  # type: ignore\n",
    "        ].reshape(n, n, -1)\n",
    "        print(f\"relative_position_bias {relative_position_bias.shape}\")\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attn=Relative_position_embedding_3d(2,2,(4,4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "def _generate_nd_grid(*sizes):\n",
    "    coords = [torch.arange(s) for s in sizes]\n",
    "    return torch.meshgrid(*coords)\n",
    "\n",
    "def local_nd_distance(*sizes, p=2.0, weights=None):\n",
    "    if weights is None:\n",
    "        weights = (1,) * len(sizes)\n",
    "    assert len(sizes) == len(weights)\n",
    "    grid = _generate_nd_grid(*sizes)\n",
    "    grid = [i.flatten() * w for i, w in zip(grid, weights)]\n",
    "    grid = torch.stack(grid, dim=1).float()\n",
    "    print(f\"ggg \\n {grid} \\n\")\n",
    "\n",
    "    d = torch.cdist(grid, grid, p=p)\n",
    "    return d\n",
    "\n",
    "local_nd_distance(2,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "spacing=(0.5,0.5,2.0)\n",
    "\n",
    "def _generate_nd_grid(*sizes):\n",
    "    coords = [torch.arange(s) for s in sizes]\n",
    "    return torch.meshgrid(*coords)\n",
    "\n",
    "def local_nd_distance_with_spacing(*sizes, p=2.0, weights=None,spacing=spacing):\n",
    "    if weights is None:\n",
    "        weights = (1,) * len(sizes)\n",
    "    assert len(sizes) == len(weights)\n",
    "    grid = _generate_nd_grid(*sizes)\n",
    "    grid = [i.flatten() * w for i, w in zip(grid, weights)]\n",
    "    grid = torch.stack(grid, dim=1).float()\n",
    "    grid[:,0]=grid[:,0]*spacing[0]\n",
    "    grid[:,1]=grid[:,1]*spacing[1]\n",
    "    grid[:,2]=grid[:,2]*spacing[2]\n",
    "    print(f\"ggg \\n {grid} \\n\")\n",
    "\n",
    "    d = torch.cdist(grid, grid, p=p)\n",
    "    return d\n",
    "\n",
    "local_nd_distance(2,2,2,spacing=spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5_path=\"/workspaces/konwersjaJsonData/explore/hdf5_loc/sparse_masks\"\n",
    "\n",
    "f = h5py.File(h5_path,'r+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['0_transp_info', '1_transp_info', '2_transp_info', '3_transp_info', 'column_indices', 'row_indices', 'row_offsets', 'shape', 'values']>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f.close()\n",
    "# f[\"32_32_32/dist_8_spacing_0.78125_0.78125_3.299999952316284/non_iso_vol\"].keys()\n",
    "# f[\"32_32_32/swin/window_4/main\"].keys()\n",
    "# f[\"32_32_32/dist_8/iso_vol\"].keys()\n",
    "\n",
    "f[\"32_32_32/swin/window_4/main\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xformers.sparse import BlockSparseTensor, SparseCSRTensor\n",
    "import xformers\n",
    "\n",
    "group_to_load=f[\"32_32_32/dist_8/iso_vol\"]\n",
    "keys=list(group_to_load.keys())\n",
    "transp_info_keys= list(filter(lambda el: 'transp_info' in el  ,keys))\n",
    "\n",
    "# transp_info_keys\n",
    "\n",
    "values=torch.tensor(group_to_load['values'][()])\n",
    "shape=tuple(group_to_load['shape'][()])\n",
    "# shape=(values.shape[0],) + shape\n",
    "\n",
    "# attn_mask = SparseCSRTensor._wrap((values.shape[0],) + shape\n",
    "attn_mask = xformers.components.attention._sputnik_sparse.SparseCS.wrap(shape=shape\n",
    ", values= values\n",
    ", row_indices= torch.tensor(group_to_load['row_indices'][()]).type(torch.int32)\n",
    ", row_offsets= torch.tensor(group_to_load['row_offsets'][()]).type(torch.int32)\n",
    ", column_indices= torch.tensor(group_to_load['column_indices'][()]).type(torch.int32)\n",
    ", _transp_info=(torch.tensor(group_to_load['0_transp_info'][()]).type(torch.int32)\n",
    "                ,torch.tensor(group_to_load['1_transp_info'][()]).type(torch.int32)\n",
    "                ,torch.tensor(group_to_load['2_transp_info'][()]).type(torch.int32)\n",
    "                ,torch.tensor(group_to_load['3_transp_info'][()]).type(torch.int32)\n",
    "                )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_nd_grid(*sizes):\n",
    "    coords = [torch.arange(s) for s in sizes]\n",
    "    return torch.meshgrid(*coords)\n",
    "\n",
    "def local_nd_distance(*sizes, p=2.0, weights=None):\n",
    "    if weights is None:\n",
    "        weights = (1,) * len(sizes)\n",
    "    assert len(sizes) == len(weights)\n",
    "    grid = _generate_nd_grid(*sizes)\n",
    "    grid = [i.flatten() * w for i, w in zip(grid, weights)]\n",
    "    grid = torch.stack(grid, dim=1).float()\n",
    "    d = torch.cdist(grid, grid, p=p)\n",
    "    return d\n",
    "\n",
    "def local_nd_pattern(*sizes, distance, p=2.0):\n",
    "    d = local_nd_distance(*sizes, p=p)\n",
    "    # print(d)\n",
    "    return d < distance\n",
    "\n",
    "# aaaa (32.0, 32.0, 32.0) distance 8\n",
    "local_mask=local_nd_pattern(32,32,32,distance=8)\n",
    "local_mask = SparseCS(local_mask, torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32768, 32768])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.equal(attn_mask.row_indices,local_mask.row_indices)\n",
    "type(attn_mask)\n",
    "attn_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32768, 32768])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5055, 0.5182, 0.5027,  ..., 0.4969, 0.5157, 0.5013],\n",
       "         [0.5041, 0.5215, 0.5003,  ..., 0.5046, 0.5138, 0.4912],\n",
       "         [0.5061, 0.5255, 0.5051,  ..., 0.4999, 0.5056, 0.4848],\n",
       "         ...,\n",
       "         [0.4872, 0.5290, 0.4844,  ..., 0.5216, 0.4983, 0.4949],\n",
       "         [0.4849, 0.5235, 0.4821,  ..., 0.5160, 0.4978, 0.4946],\n",
       "         [0.4909, 0.5128, 0.4824,  ..., 0.5163, 0.5119, 0.4969]],\n",
       "\n",
       "        [[0.4823, 0.4939, 0.5164,  ..., 0.4930, 0.4992, 0.4844],\n",
       "         [0.4717, 0.4972, 0.5296,  ..., 0.4877, 0.4997, 0.4947],\n",
       "         [0.4736, 0.5009, 0.5257,  ..., 0.4865, 0.5004, 0.4903],\n",
       "         ...,\n",
       "         [0.4997, 0.4825, 0.4790,  ..., 0.5088, 0.5033, 0.4953],\n",
       "         [0.4906, 0.4845, 0.4810,  ..., 0.5077, 0.5130, 0.5011],\n",
       "         [0.4935, 0.4770, 0.4709,  ..., 0.5123, 0.5136, 0.4991]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xformers.factory import xFormerEncoderBlock, xFormerEncoderConfig\n",
    "from xformers.factory.model_factory import xFormer, xFormerConfig\n",
    "from xformers.components import MultiHeadDispatch, build_attention\n",
    "from xformers.sparse import BlockSparseTensor, SparseCSRTensor\n",
    "\n",
    "from xformers.components.feedforward.fused_mlp import FusedMLP\n",
    "\n",
    "DROPOUT=0.0\n",
    "\n",
    "SEQ=32768\n",
    "EMB= 12\n",
    "num_heads=2\n",
    "my_config = {\n",
    "    \"name\": \"scaled_dot_product\",  # you can easily make this dependent on a file, sweep,..\n",
    "    \"dropout\": DROPOUT,\n",
    "    \"seq_len\": SEQ,\n",
    "    \"attention_query_mask\": None#torch.rand((SEQ, 1)) < 0.3, # some dummy mask\n",
    "}\n",
    "# attention = xops.memory_efficient_attention\n",
    "\n",
    "attention=build_attention(my_config)\n",
    "\n",
    "# build a multi head dispatch to test this attention mechanism\n",
    "multi_head = MultiHeadDispatch(\n",
    "    seq_len=SEQ,\n",
    "    dim_model=EMB,\n",
    "    residual_dropout=DROPOUT,\n",
    "    num_heads=num_heads,\n",
    "    attention=attention,\n",
    ")\n",
    "q=torch.rand((2,SEQ,EMB))\n",
    "k=torch.rand((2,SEQ,EMB))\n",
    "v=torch.rand((2,SEQ,EMB))\n",
    "\n",
    "# attention(q,k,v,att_mask=attn_mask)#,att_mask=attn_mask\n",
    "attention(q,k,v,att_mask=attn_mask)#,att_mask=attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "from xformers.components.attention.core import _matmul_with_mask\n",
    "mask= torch.zeros((4,4))\n",
    "q= torch.ones((4,4))\n",
    "k= torch.eye(4)\n",
    "att = _matmul_with_mask(q, k.transpose(-2, -1), mask)\n",
    "att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::as_strided' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31034 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:43986 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:26824 [kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:929 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:20475 [kernel]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\nNegative: fallthrough registered at ../aten/src/ATen/native/NegateFallback.cpp:23 [kernel]\nZeroTensor: registered at aten/src/ATen/RegisterZeroTensor.cpp:161 [kernel]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4733 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16728 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:487 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:819 [kernel]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1077 [kernel]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_60757/1226745001.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# spp.coalesce().values()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mspp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::as_strided' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31034 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:43986 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:26824 [kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:929 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:20475 [kernel]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\nNegative: fallthrough registered at ../aten/src/ATen/native/NegateFallback.cpp:23 [kernel]\nZeroTensor: registered at aten/src/ATen/RegisterZeroTensor.cpp:161 [kernel]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4733 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16728 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:487 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:819 [kernel]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1077 [kernel]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import einops\n",
    "bb=torch.zeros((3,3)).bool()\n",
    "bb[1,1]=True\n",
    "bb[0,1]=True\n",
    "bb[1,2]=True\n",
    "cc= torch.ones((3,3))*2\n",
    "cc[1,1]=3\n",
    "indicies=torch.argwhere(bb)\n",
    "indicies=einops.rearrange(indicies,'a b -> b a')\n",
    "\n",
    "spp=torch.sparse_coo_tensor(indices=indicies, values=cc[bb])\n",
    "# spp\n",
    "\n",
    "# spp.coalesce().values()\n",
    "new_matrix = torch.stack([matrix for _ in range(4)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 1, 1],\n",
       "                       [2, 0, 2]]),\n",
       "       values=tensor([3., 4., 5.]),\n",
       "       size=(2, 4), nnz=3, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.tensor([[0, 1, 1],\n",
    "                   [2, 0, 2]])\n",
    "v = torch.tensor([3, 4, 5], dtype=torch.float32)\n",
    "torch.sparse_coo_tensor(i, v, [2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
