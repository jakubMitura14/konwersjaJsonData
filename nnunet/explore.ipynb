{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "from subprocess import Popen\n",
    "import subprocess\n",
    "import SimpleITK as sitk\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import functools\n",
    "from functools import partial\n",
    "import sys\n",
    "import os.path\n",
    "from os import path as pathOs\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import shutil\n",
    "from os.path import basename, dirname, exists, isdir, join, split\n",
    "from pathlib import Path\n",
    "import fileinput\n",
    "import re\n",
    "import subprocess\n",
    "from toolz.itertoolz import groupby\n",
    "import seaborn as sns\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import SimpleITK as sitk\n",
    "import mdai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pydicom\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import functools\n",
    "from functools import partial\n",
    "import mdai\n",
    "import math\n",
    "import time\n",
    "import itertools\n",
    "from pydicom.fileset import FileSet\n",
    "from os import path as pathOs\n",
    "from pathlib import Path\n",
    "import toolz\n",
    "from toolz.curried import pipe, map, filter, get\n",
    "from toolz import curry\n",
    "from os.path import basename, dirname, exists, isdir, join, split\n",
    "import nnunetv2\n",
    "\n",
    "import elastixRegister as elastixRegister\n",
    "from elastixRegister import reg_a_to_b\n",
    "import tempfile\n",
    "import shutil\n",
    "import re\n",
    "from toolz.itertoolz import groupby\n",
    "from toolz import curry\n",
    "# import multiprocess\n",
    "# p = multiprocess.Pool(os.cpu_count())\n",
    "import multiprocessing as mp\n",
    "import json\n",
    "import os\n",
    "from subprocess import Popen\n",
    "import subprocess\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "def copy_changing_type(source, dest):\n",
    "    image= sitk.ReadImage(source)\n",
    "    # nan_count=np.sum(np.isnan(np.array(sitk.GetArrayFromImage(image)).flatten()))\n",
    "    # if(nan_count>0):\n",
    "    #     raise ValueError(f\"!!! nan in {source}\")\n",
    "    image = sitk.DICOMOrient(image, 'LPS')\n",
    "    image.SetDirection((1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)) \n",
    "    image=sitk.Cast(image, sitk.sitkFloat32)\n",
    "    writer = sitk.ImageFileWriter() \n",
    "    writer.SetFileName(dest)\n",
    "    writer.Execute(image)\n",
    "    return dest\n",
    "\n",
    "#metadata directory\n",
    "resCSVDir='/home/sliceruser/workspaces/konwersjaJsonData/outCsv/resCSV.csv'\n",
    "#directory with inferred prostates\n",
    "dir_inferred_prost='/workspaces/konwersjaJsonData/explore/my_prost_infered'\n",
    "sourceFrame = pd.read_csv(resCSVDir)\n",
    "test_ids = pd.read_csv('/workspaces/konwersjaJsonData/test_ids.csv' )\n",
    "new_col_name= 'inferred_pg'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_id_from_file_name(path_str):\n",
    "    path_str=path_str.replace('.nii.gz','')\n",
    "    path_str=path_str[1:5]\n",
    "    return int(path_str)\n",
    "\n",
    "def add_t2w_to_name(source):\n",
    "    if(source==' '):\n",
    "        return ' '\n",
    "    if('t2w' in source):\n",
    "        return source\n",
    "    new_path= source.replace('.nii.gz','_t2w.nii.gz')\n",
    "    copy_changing_type(source, new_path)\n",
    "    return new_path\n",
    "\n",
    "def add_inferred_full_prost_to_dataframe(dir_inferred_prost, df,new_col_name):\n",
    "    \"\"\" \n",
    "    we have some inferred anatomical segmentations done by previous \n",
    "    models now we want to take the folder with \n",
    "    \"\"\"\n",
    "    list_files= os.listdir(dir_inferred_prost)\n",
    "    list_files= list(filter(lambda el : el[0]=='9' ,list_files ))\n",
    "    list_ids= list(map(get_id_from_file_name,list_files))\n",
    "    list_files= list(map( lambda el: f\"{dir_inferred_prost}/{el}\" ,list_files))\n",
    "    file_and_id= dict(list(zip(list_ids,list_files)))\n",
    "    new_col_dat= list(map( lambda el: file_and_id.get(el,' ') ,df['masterolds'].to_numpy() ))\n",
    "    #changing path name to mark it is t2w related\n",
    "    new_col_dat= list(map(add_t2w_to_name,new_col_dat))\n",
    "\n",
    "    df[new_col_name]=new_col_dat\n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathh=\"/home/sliceruser/nnunetMainFolder/nnUNet_preprocessed/Dataset294_Prostate/nnUNetPlans_3d_lowres/9002700_seg.npy\"\n",
    "# arr=np.load(pathh)\n",
    "# for i in range(5,40):\n",
    "#     to_print=arr[0,i,:,:]\n",
    "#     print(f\" to_print {to_print.shape}\")\n",
    "#     sns.heatmap(to_print)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols=sourceFrame.columns\n",
    "noSegCols=list(filter(lambda el: '_noSeg' in el , cols))+['series_MRI_path']\n",
    "lesion_cols=list(filter(lambda el: 'lesion' in el , noSegCols))\n",
    "\n",
    "sourceFrame=add_inferred_full_prost_to_dataframe(dir_inferred_prost, sourceFrame,new_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def groupByMaster(rowws):\n",
    "    grouped_by_master= groupby(lambda row : row[1]['masterolds'],rowws)\n",
    "    # grouped_by_master=[(key,list(group)) for key, group in grouped_by_master]\n",
    "    return dict(grouped_by_master).items()\n",
    "\n",
    "\n",
    "\n",
    "def get_bool_arr_from_path(pathh):\n",
    "    \"\"\"\n",
    "    given path reads it and return associated array\n",
    "    then it casts it to boolean data type\n",
    "    \"\"\"\n",
    "    imageA=sitk.ReadImage(pathh)\n",
    "    return sitk.GetArrayFromImage(imageA).astype(bool)\n",
    "\n",
    "\n",
    "def getPathsFromRow(row,list_columns):\n",
    "    \"\"\"\n",
    "    extracting all paths of intrest from row\n",
    "    \"\"\"\n",
    "    res=  map( lambda colName : (colName,row[1][colName] ),list_columns )\n",
    "    return res\n",
    "\n",
    "def getListModality(modalityName,pathhs,non_mri_inputs):\n",
    "    \"\"\"\n",
    "    getting paths related to single modality and extracting main MRI image\n",
    "    non_mri_inputs - some inputs that are designed to be put into input channels \n",
    "    \"\"\"\n",
    "    if(modalityName not in non_mri_inputs):\n",
    "        # we are intrested only in paths that has the prostate segmentation\n",
    "        pathhs=list(map(lambda el: el[1],pathhs))\n",
    "        # pathhs= list(filter(lambda el :\"pg_t2w.nii.gz\" not in el , pathhs))\n",
    "        mod_paths = list(filter(lambda pathh :modalityName in  pathh,pathhs))\n",
    "        mri = list(filter(lambda el: '.mha' in el ,mod_paths))\n",
    "        if(len(mri)==0):\n",
    "            return ' ',[]\n",
    "        mri=mri[0]   \n",
    "        mod_paths= list(filter(lambda pathh: '.mha' not in pathh , mod_paths))\n",
    "        return (modalityName,(mri,np.unique(mod_paths).tolist()))\n",
    "    \n",
    "    elif(modalityName in non_mri_inputs):\n",
    "        # colNames=list(map(lambda el: el[0],pathhs))\n",
    "        pathhss= list(filter(lambda el :modalityName in el[0] , pathhs))   \n",
    "        if(len(pathhss)==0):\n",
    "            return ' ',[]        \n",
    "        res= (modalityName, (modalityName,np.unique(pathhss[0][1]).tolist())  )\n",
    "        return res\n",
    "\n",
    "\n",
    "def myFlatten(liist):\n",
    "    return  itertools.chain(*liist)\n",
    "\n",
    "def map_modalities(pathhs,modalities,non_mri_inputs):\n",
    "    all_modalities=modalities+non_mri_inputs\n",
    "    res= toolz.pipe(all_modalities\n",
    "                ,map(partial(getListModality,pathhs=pathhs,non_mri_inputs=non_mri_inputs))\n",
    "                ,list\n",
    "            )\n",
    "    # print(f\"gggg {res}\")\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def iterGroupModalities(groupTuple,modalities_of_intrest,label_cols,non_mri_inputs ):\n",
    "    \"\"\"\n",
    "    grouping the paths into dictionary relative to modalities they represent and lesions on thise \n",
    "    modalities\n",
    "    \"\"\"\n",
    "    masterOlds, listRows= groupTuple\n",
    "    pathhs=toolz.pipe(listRows\n",
    "                ,map(partial(getPathsFromRow,list_columns=np.unique(label_cols+['series_MRI_path']+non_mri_inputs)))\n",
    "                ,myFlatten\n",
    "                # ,filter(lambda el : len(el)>2)\n",
    "                ,list\n",
    "                ,partial(map_modalities,modalities=modalities_of_intrest,non_mri_inputs=non_mri_inputs)\n",
    "                ,dict\n",
    "                )   \n",
    "    return (masterOlds,pathhs)\n",
    "\n",
    "\n",
    "\n",
    "def get_bool_or(pathA,pathB):\n",
    "    if(isinstance(pathA, str) and isinstance(pathB, str)):\n",
    "        return np.logical_or(get_bool_arr_from_path(pathA),get_bool_arr_from_path(pathB))\n",
    "    elif(isinstance(pathA, str) and not isinstance(pathB, str)):\n",
    "        return np.logical_or(get_bool_arr_from_path(pathA),pathB)\n",
    "    elif(not isinstance(pathA, str) and isinstance(pathB, str)):\n",
    "        return np.logical_or(get_bool_arr_from_path(pathB),pathA)\n",
    "    else:\n",
    "        return np.logical_or(pathB,pathA)\n",
    "\n",
    "def get_bool_and(pathA,pathB):\n",
    "    if(isinstance(pathA, str) and isinstance(pathB, str)):\n",
    "        return np.logical_and(get_bool_arr_from_path(pathA),get_bool_arr_from_path(pathB))\n",
    "    elif(isinstance(pathA, str) and not isinstance(pathB, str)):\n",
    "        return np.logical_and(get_bool_arr_from_path(pathA),pathB)\n",
    "    elif(not isinstance(pathA, str) and isinstance(pathB, str)):\n",
    "        return np.logical_and(get_bool_arr_from_path(pathB),pathA)\n",
    "    else:\n",
    "        return np.logical_and(pathB,pathA)\n",
    "\n",
    "\n",
    "def get_4_id(masterolds):\n",
    "    \"\"\"\n",
    "    take master id and changes it into string that starts with 0s and have always length 4\n",
    "    \"\"\"\n",
    "    masteroldsStand=str(masterolds)\n",
    "    if(len(masteroldsStand)==1):\n",
    "        return f\"000{masteroldsStand}\"\n",
    "    elif(len(masteroldsStand)==2):\n",
    "        return f\"00{masteroldsStand}\"\n",
    "    elif(len(masteroldsStand)==3):\n",
    "        return f\"0{masteroldsStand}\"\n",
    "    return masteroldsStand\n",
    "\n",
    "def save_from_arr(zeroArray,image3D,newPathLab):\n",
    "    \"\"\"\n",
    "    given array saves it to file into defined path using simpleitk\n",
    "    \"\"\"\n",
    "    writer = sitk.ImageFileWriter()\n",
    "    image = sitk.GetImageFromArray(zeroArray.astype(float).astype(np.uint8))  \n",
    "    nan_count=np.sum(np.isnan(np.array(sitk.GetArrayFromImage(image)).flatten()))\n",
    "    if(nan_count>0):\n",
    "        raise ValueError(f\"!!! nan in image would be saved as {newPathLab}\")\n",
    "\n",
    "    image.SetSpacing(image3D.GetSpacing())\n",
    "    image.SetOrigin(image3D.GetOrigin())\n",
    "    image.SetDirection(image3D.GetDirection())   \n",
    "    image = sitk.DICOMOrient(image, 'LPS')\n",
    "    image.SetDirection((1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)) \n",
    "    writer.SetFileName(newPathLab)\n",
    "    writer.Execute(image)\n",
    "\n",
    "def copy_changing_type(source, dest):\n",
    "    image= sitk.ReadImage(source)\n",
    "    # nan_count=np.sum(np.isnan(np.array(sitk.GetArrayFromImage(image)).flatten()))\n",
    "    # if(nan_count>0):\n",
    "    #     raise ValueError(f\"!!! nan in {source}\")\n",
    "    image = sitk.DICOMOrient(image, 'LPS')\n",
    "    image.SetDirection((1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)) \n",
    "    image=sitk.Cast(image, sitk.sitkFloat32)\n",
    "    writer = sitk.ImageFileWriter() \n",
    "    writer.SetFileName(dest)\n",
    "    writer.Execute(image)\n",
    "    return dest\n",
    "\n",
    "# mod=\"adc\"\n",
    "def get_key_by_value(mod,channel_names):\n",
    "    return list(channel_names.keys())[list(channel_names.values()).index(mod)]\n",
    "\n",
    "def prepare_out_paths(group,modalities_of_intrest,labelsTrFolder,imagesTrFolder,non_mri_inputs,channel_names ):\n",
    "    #preparing names\n",
    "    for_id=get_4_id(group[0])\n",
    "    label_new_path= join(labelsTrFolder,f\"9{for_id}00.nii.gz\" )\n",
    "    # prostate_path=join(imagesTrFolder,f\"9{for_id}00_000{3}.nii.gz\" )\n",
    "    out_pathsDict= list(map( lambda mod:(mod,join(imagesTrFolder,f\"9{for_id}00_000{get_key_by_value(mod,channel_names)}.nii.gz\" )) ,np.unique(modalities_of_intrest+non_mri_inputs) ))\n",
    "    out_pathsDict=dict(out_pathsDict)\n",
    "    return label_new_path,out_pathsDict\n",
    "\n",
    "# with mp.Pool(processes = mp.cpu_count()) as pool:\n",
    "# #with mp.Pool(processes = 1) as pool:\n",
    "#     @curry  \n",
    "#     def pmap(fun,iterable):\n",
    "#         return pool.map(fun,iterable)\n",
    "    \n",
    "def reg_a_to_b_by_metadata_single_c(fixed_image_path,moving_image_path,interpolator):\n",
    "    # print(f\"fixed_image_path {fixed_image_path} moving_image_path {moving_image_path}\")\n",
    "    # moving_image_path=moving_image_path[0]\n",
    "    fixed_image=sitk.ReadImage(fixed_image_path)\n",
    "    moving_image=sitk.ReadImage(moving_image_path)\n",
    "\n",
    "    # fixed_image=sitk.Cast(fixed_image, sitk.sitkUInt8)\n",
    "    # moving_image=sitk.Cast(moving_image, sitk.sitkInt)\n",
    "    \n",
    "    arr=sitk.GetArrayFromImage(moving_image)\n",
    "    resampled=sitk.Resample(moving_image, fixed_image, sitk.Transform(3, sitk.sitkIdentity), interpolator, 0)\n",
    "    return sitk.GetArrayFromImage(resampled)\n",
    "    \n",
    "def reg_a_to_b_by_metadata_single_b(fixed_image_path,moving_image_path,out_folder, interpolator=sitk.sitkNearestNeighbor):\n",
    "    if(len(moving_image_path)<4):\n",
    "        moving_image_path=moving_image_path[0]\n",
    "    fixed_image=sitk.ReadImage(fixed_image_path)\n",
    "    moving_image=sitk.ReadImage(moving_image_path)\n",
    "\n",
    "    # fixed_image=sitk.Cast(fixed_image, sitk.sitkUInt8)\n",
    "    # moving_image=sitk.Cast(moving_image, sitk.sitkInt)\n",
    "    \n",
    "    arr=sitk.GetArrayFromImage(moving_image)\n",
    "    resampled=sitk.Resample(moving_image, fixed_image, sitk.Transform(3, sitk.sitkIdentity), interpolator, 0)\n",
    "    \n",
    "    # print(f\" prim sum {np.sum(sitk.GetArrayFromImage(sitk.ReadImage(moving_image_path)).flatten())} \\n suuum {np.sum(sitk.GetArrayFromImage(resampled).flatten())} \")\n",
    "  \n",
    "    writer = sitk.ImageFileWriter()\n",
    "    new_path= join(out_folder,moving_image_path.split('/')[-1])\n",
    "    writer.SetFileName(new_path)\n",
    "    writer.Execute(resampled)\n",
    "\n",
    "    return new_path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_crop(image,min_z,min_y,min_x,max_z,max_x,max_y):\n",
    "    size=[int(max_y-min_y)-1,int(max_x-min_x)-1,int(max_z-min_z)-1]\n",
    "    beg=[int(min_y),int(min_x),int(min_z)]\n",
    "    # size=[int(max_x-min_x)-1,int(max_y-min_y)-1,int(max_z-min_z)-1]\n",
    "    # beg=[int(min_x), int(min_y),int(min_z)]\n",
    "    extract = sitk.ExtractImageFilter()\n",
    "    extract.SetSize(size)\n",
    "    extract.SetIndex(beg)\n",
    "    extracted_image = extract.Execute(image)\n",
    "    return extracted_image\n",
    "\n",
    "def my_concat(grouped):\n",
    "    # grouped= list(map(lambda tupl: tupl[1],grouped))\n",
    "    res=np.stack(grouped).astype(int)\n",
    "    res=np.sum(res,axis=0)\n",
    "    return res\n",
    "\n",
    "def get_from_arr(zeroArray,image3D):\n",
    "    \"\"\"\n",
    "    given array saves it to file into defined path using simpleitk\n",
    "    \"\"\"\n",
    "    image = sitk.GetImageFromArray(zeroArray.astype(float).astype(np.uint8))  \n",
    "    # nan_count=np.sum(np.isnan(np.array(sitk.GetArrayFromImage(image)).flatten()))\n",
    "    # if(nan_count>0):\n",
    "    #     raise ValueError(f\"!!! nan in image would be saved as {newPathLab}\")\n",
    "\n",
    "    image.SetSpacing(image3D.GetSpacing())\n",
    "    image.SetOrigin(image3D.GetOrigin())\n",
    "    image.SetDirection(image3D.GetDirection())   \n",
    "    image = sitk.DICOMOrient(image, 'LPS')\n",
    "    return image\n",
    "\n",
    "\n",
    "def augment_two_channel(dat_curr,target_curr):\n",
    "    \"\"\"\n",
    "    adding in random spots (but not where already some labels are) lesions that are \n",
    "    either low or high on both adc and hbv (they should be high hbv low adc)\n",
    "    function is not batched\n",
    "    \"\"\"\n",
    "    img_shape= (dat_curr.shape[1],dat_curr.shape[2],dat_curr.shape[3])\n",
    "    #checked for lesions \n",
    "    # #mean adc is 1094.62154301315 stdev 481.980487471925\n",
    "    #hbv mean 19.4776261112059 std 4.75021004238441\n",
    "    #for prostate no lesions\n",
    "    #adc mean 1169.06275422957  std 123.857216768034\n",
    "    #hbv mean 13.8230222890104  std 3.02186134116448\n",
    "\n",
    "    gauss_vals=np.array([ [[1094.62154301315,30.0],[13.8230222890104,3.02186134116448 ] ]\n",
    "                         ,[[1169.06275422957,30.0],[19.4776261112059,4.7502100423844] ] ])\n",
    "    #n - controlling how big the pseudo lesion will be\n",
    "    n=np.random.randint(2,5)\n",
    "    #k - controlling the numebr of pseudo lesions\n",
    "    k=np.random.randint(0,20) \n",
    "    #1) get the target and dilatate it n+1 times (if its sum is above 0)\n",
    "    target_big= ndimage.binary_dilation((target_curr>0),iterations=n)\n",
    "    #2) get indicies where dilatated target is still zero and choose random k indicies from it\n",
    "    target_big=np.argwhere(np.logical_not(target_big))\n",
    "    rng = np.random.default_rng()\n",
    "    rng.shuffle(target_big,axis=0)\n",
    "    target_big=target_big[0:k,:]\n",
    "    #3) create new zero bool array of original size \n",
    "    res_bool= np.zeros(img_shape).astype(bool)\n",
    "    #4) in a neww array set chosen points to True\n",
    "    res_bool[target_big[:,0],target_big[:,1],target_big[:,2]]=True\n",
    "    #5) perform binary dilatation n-1 times and save\n",
    "    res_bool_a= ndimage.binary_dilation(res_bool,iterations=n-1)\n",
    "    #6) perform last dilatation - and find indicies that were added in last dilatation\n",
    "    res_bool_b= ndimage.binary_dilation(res_bool_a,iterations=1)\n",
    "    diff= np.argwhere(np.logical_and(np.logical_not(res_bool_a), res_bool_b))\n",
    "    #7) set randomly 60% of them to false so we will have more realistic border\n",
    "    rng = np.random.default_rng()\n",
    "    rng.shuffle(diff,axis=0)\n",
    "    len= diff.shape[0]\n",
    "    diff=diff[0:len,:]\n",
    "    res_bool_b[diff[:,0],diff[:,1],diff[:,2]]=False\n",
    "    #8) we get resulting boolean array calling it res_bool\n",
    "    res_bool=res_bool_b\n",
    "    #9) we index image (both channels) with res bool and set it to 0\n",
    "    data_a=dat_curr[0,:,:,:]\n",
    "    data_b=dat_curr[1,:,:,:]\n",
    "    data_a[res_bool]=0\n",
    "    data_b[res_bool]=0\n",
    "    # dat_curr=np.stack([data_a,data_b])\n",
    "\n",
    "    #10) we create new float array of the size like image\n",
    "    #11) we set it with either mean and variance typical for lesions on hbv or adc and we set the same values \n",
    "        # for both channels\n",
    "    rng = np.random.default_rng()    \n",
    "    rng.shuffle(gauss_vals,axis=0)\n",
    "    noise_adc=np.random.normal(gauss_vals[0,0,0], gauss_vals[0,0,1], size=img_shape)\n",
    "    noise_hbv=np.random.normal(gauss_vals[0,1,0], gauss_vals[0,1,1], size=img_shape)\n",
    "    #12) we set evrything outside of res_bool to false    \n",
    "    noise_adc[np.logical_not(res_bool)]=0.0\n",
    "    noise_hbv[np.logical_not(res_bool)]=0.0\n",
    "    noise= np.stack([noise_adc,noise_hbv])\n",
    "    #13)we add image (with zeroad indexes) to array we got in previous step\n",
    "    dat_curr=np.stack([data_a,data_b])\n",
    "    dat_curr=dat_curr+noise\n",
    "    return dat_curr\n",
    "            #     dat_curr= data[bi,:,:,:,:]\n",
    "            # target_curr= data[bi,:,:,:]\n",
    "\n",
    "def visualize_range(group,main_modality,modalities_of_intrest,non_mri_inputs,out_folder):\n",
    "\n",
    "    modalit_path_add= list(map( lambda el:(group[1][el]) ,non_mri_inputs))\n",
    "    # print(f\"ggg group[1] {group[1]}\")\n",
    "    # print(f\"modalit_path_add[0][1] {modalit_path_add[0][1]}\")\n",
    "    if(modalit_path_add[0][1][0]==' '):\n",
    "        print(f\"no prostate! {group[0]}\")\n",
    "        return group[0]\n",
    "\n",
    "    modalities_of_intrest_without_main= ['hbv']\n",
    "    # modalities_of_intrest_without_main=non_mri_inputs+modalities_of_intrest_without_main\n",
    "\n",
    "    sources_dict=group[1]\n",
    "    sources_dict[non_mri_inputs[0]]=(modalit_path_add[0][1][0],)\n",
    "\n",
    "\n",
    "    registered_modalities_arrs= list(map(lambda mod: reg_a_to_b_by_metadata_single_c(sources_dict[main_modality][0],sources_dict[mod][0], sitk.sitkBSpline)                                    \n",
    "                                                      ,modalities_of_intrest_without_main ))\n",
    "    registered_prostate= list(map(lambda mod: reg_a_to_b_by_metadata_single_c(sources_dict[main_modality][0],sources_dict[mod][0], sitk.sitkNearestNeighbor)                                    \n",
    "                                                      ,non_mri_inputs ))\n",
    "\n",
    "    adc_arr=sitk.GetArrayFromImage(sitk.ReadImage(group[1][main_modality][0]))\n",
    "    prostate_arr= registered_prostate[0]\n",
    "    hbv_arr= registered_modalities_arrs[0]\n",
    "\n",
    "    ########### manage labels\n",
    "\n",
    "    labels_hbv=group[1]['hbv'][1]\n",
    "    labels_adc=group[1][main_modality][1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    labels_hbv=list(map(lambda pathh_moving: reg_a_to_b_by_metadata_single_b(sources_dict[main_modality][0],pathh_moving,out_folder,sitk.sitkNearestNeighbor)                                    \n",
    "                                                      ,labels_hbv ))\n",
    "\n",
    "    labels= labels_adc+labels_hbv\n",
    "    labRes= np.zeros_like(adc_arr)\n",
    "    if(len(labels)>0):\n",
    "\n",
    "        print(f\"labels {labels}\")\n",
    "        label_names=list(map(lambda pathh:  \n",
    "                            list(filter( lambda el_out:'/' not in el_out ,\n",
    "                                list(filter(lambda el: 'lesion' in el  ,pathh.split('_')))))[0]\n",
    "                            \n",
    "                            ,labels ))\n",
    "\n",
    "        label_names= list(filter( lambda el: '/' not in el ,label_names))\n",
    "        # we want to get sum of all labels of the same name - so lesion 1 sum lesion 2 sum ...\n",
    "        # then we encode it in separate file \n",
    "        zipped_names= list(zip(label_names,labels))\n",
    "\n",
    "        grouped_by_lesion_num = dict(groupby(lambda tupl :tupl[0],zipped_names)).items()\n",
    "        grouped_by_lesion_num= list(map(lambda grouped :  (grouped[0],list(map(lambda el: el[1],grouped[1])) ) , grouped_by_lesion_num  ))\n",
    "        # print(f\"\\n grouped_by_lesion_num {grouped_by_lesion_num} \\n\")\n",
    "\n",
    "        # we need to get the paths to bool arrs and sum it all\n",
    "        # next we keep all >1\n",
    "        # krowa set it to agreement of 2 annotators\n",
    "        # print(f\"grouped_by_lesion_num {grouped_by_lesion_num}\")\n",
    "        grouped_by_lesion_num_arrs= list(map(lambda grouped : \n",
    "                                              list(map(get_bool_arr_from_path, grouped[1]))\n",
    "                                                ,grouped_by_lesion_num  ))\n",
    "\n",
    "        grouped_by_lesion_num_arrs= list(map(my_concat,grouped_by_lesion_num_arrs  ))\n",
    "\n",
    "        grouped_by_lesion_num_arrs= np.stack(grouped_by_lesion_num_arrs,axis=0).astype(int)\n",
    "\n",
    "        reduced_common= (np.sum(grouped_by_lesion_num_arrs,axis=0)>1).astype(int)\n",
    "\n",
    "        reduced_sum = np.array(functools.reduce(get_bool_or, labels)).astype(bool)\n",
    "        reduced_sum= ndimage.binary_dilation(reduced_sum,iterations=3)\n",
    "\n",
    "        reduced_common_eroded= ndimage.binary_erosion(reduced_common,iterations=1)\n",
    "        summ=np.sum(reduced_common.flatten())\n",
    "        if(summ>0):\n",
    "            ratio=(np.sum(reduced_common_eroded.flatten()) /np.sum(reduced_common.flatten()))\n",
    "            print(f\"ratio {ratio}\")\n",
    "            if(ratio>0.15 ):\n",
    "                reduced_common=reduced_common_eroded\n",
    "\n",
    "        reduced_common_eroded= ndimage.binary_erosion(reduced_common,iterations=1)\n",
    "        if(summ>0):\n",
    "            ratio=(np.sum(reduced_common_eroded.flatten()) /np.sum(reduced_common.flatten()))\n",
    "            print(f\"ratio {ratio}\")\n",
    "            if(ratio>0.15 ):\n",
    "                reduced_common=reduced_common_eroded\n",
    "\n",
    "\n",
    "        print(f\"reduced_sum {np.sum(reduced_sum.flatten())}  reduced_common {np.sum(reduced_common.flatten())}\")\n",
    "        labRes=reduced_sum#(reduced_sum>0).astype(int)\n",
    "\n",
    "        labRes=labRes+(reduced_common.astype(int))\n",
    "    # label_names=list(map(lambda pathh:  \n",
    "    #                      list(filter( lambda el_out:'/' not in el_out ,\n",
    "    #                         list(filter(lambda el: 'lesion' in el  ,pathh.split('_')))))[0]\n",
    "                         \n",
    "    #                      ,labels ))\n",
    "    # label_names= list(filter( lambda el: '/' not in el ,label_names))\n",
    "    # # we want to get sum of all labels of the same name - so lesion 1 sum lesion 2 sum ...\n",
    "    # # then we encode it in separate file \n",
    "    # zipped_names= list(zip(label_names,labels))\n",
    "\n",
    "    # grouped_by_lesion_num = dict(groupby(lambda tupl :tupl[0],zipped_names)).items()\n",
    "    # grouped_by_lesion_num= list(map(lambda grouped :  (grouped[0],list(map(lambda el: el[1],grouped[1])) ) , grouped_by_lesion_num  ))\n",
    "    # print(f\"\\n grouped_by_lesion_num {grouped_by_lesion_num} \\n\")\n",
    "    # grouped_by_lesion_num_arrs= list(map(lambda grouped :  (grouped[0],np.array(functools.reduce(get_bool_or, grouped[1]))),grouped_by_lesion_num  ))\n",
    "    # label_names_uniq=list(map(lambda el: el[0] ,grouped_by_lesion_num))\n",
    "    # label_nums= list(map(lambda name: \"\".join(list(filter(lambda el:el.isdigit(),name)))  ,label_names_uniq))\n",
    "\n",
    "\n",
    "    # grouped_by_lesion_num_arrs= list(map(lambda arr ,ndimage.binary_dilation(arr,iterations=1),grouped_by_lesion_num_arrs))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    prostate_arr_indicies= np.argwhere(prostate_arr)\n",
    "\n",
    "\n",
    "\n",
    "    add_padd_z=50\n",
    "    add_padd_x=14\n",
    "    add_padd_y=14\n",
    "\n",
    "    max_z= np.max(prostate_arr_indicies[:,0])+add_padd_z\n",
    "    min_z= np.min(prostate_arr_indicies[:,0])-add_padd_z\n",
    "    # min_z=0\n",
    "    # max_z=adc_arr.shape[0]\n",
    "    max_x= np.max(prostate_arr_indicies[:,1])+add_padd_x\n",
    "    min_x= np.min(prostate_arr_indicies[:,1])-add_padd_x\n",
    "\n",
    "    max_y= np.max(prostate_arr_indicies[:,2])+add_padd_y\n",
    "    min_y= np.min(prostate_arr_indicies[:,2])-add_padd_y\n",
    "    \n",
    "    min_z=max(min_z,0)\n",
    "    min_y=max(min_y,0)\n",
    "    min_x=max(min_x,0)\n",
    "\n",
    "    max_z=min(max_z,prostate_arr.shape[0]-1)\n",
    "    max_x=min(max_x,prostate_arr.shape[1]-1)\n",
    "    max_y=min(max_y,prostate_arr.shape[2]-1)\n",
    "\n",
    "\n",
    "    # print(f\"max_z {max_z} min_z {min_z} max_x {max_x} min_x {min_x} max_y {max_y} min_y {min_y}\")\n",
    "\n",
    "    adc_image = sitk.ReadImage(group[1][main_modality][0])\n",
    "    hbv_image = get_from_arr(hbv_arr,adc_image)\n",
    "    label_image = get_from_arr(labRes,adc_image)\n",
    "\n",
    "\n",
    "    adc_image=my_crop(adc_image,min_z,min_y,min_x,max_z,max_x,max_y)\n",
    "    hbv_image=my_crop(hbv_image,min_z,min_y,min_x,max_z,max_x,max_y)\n",
    "    label_image=my_crop(label_image,min_z,min_y,min_x,max_z,max_x,max_y)\n",
    "\n",
    "    adc_arr=sitk.GetArrayFromImage(adc_image)\n",
    "    hbv_arr=sitk.GetArrayFromImage(hbv_image)\n",
    "    labRes=sitk.GetArrayFromImage(label_image)\n",
    "    # print(f\"fffffff adc_arr{adc_arr.shape}  hbv_arr {hbv_arr.shape}\")\n",
    "\n",
    "    stacked =np.stack([adc_arr,hbv_arr])\n",
    "    # augment_two_channel(dat_curr,target_curr)\n",
    "\n",
    "    dat=augment_two_channel(stacked,labRes)\n",
    "    adc_arr=dat[0,:,:,:]\n",
    "    hbv_arr=dat[1,:,:,:]\n",
    "\n",
    "    twos=(labRes==2)\n",
    "    # print(f\"aaa adc {np.mean(adc_arr[twos])} hbv {np.mean(hbv_arr[twos])}  \")\n",
    "\n",
    "    # min_x=0\n",
    "    # max_x=adc_arr.shape[1]\n",
    "\n",
    "    # min_y=0\n",
    "    # max_y=adc_arr.shape[2]\n",
    "\n",
    "    # adc_arr=adc_arr[min_z:max_z,min_x:max_x,min_y:max_y]\n",
    "    # hbv_arr=hbv_arr[min_z:max_z,min_x:max_x,min_y:max_y]\n",
    "    # prostate_arr=prostate_arr[min_z:max_z,min_x:max_x,min_y:max_y]\n",
    "    # labRes=labRes[min_z:max_z,min_x:max_x,min_y:max_y]\n",
    "    labRes_bool=labRes>1\n",
    "\n",
    "    big_prost_z=einops.reduce(labRes_bool,'z x y-> z','sum')\n",
    "    slice=np.argmax(big_prost_z)\n",
    "\n",
    "    big_prost_x=einops.reduce(prostate_arr,'z x y-> x','sum')\n",
    "    on_x=np.argmax(big_prost_z)\n",
    "\n",
    "    \n",
    "    prost= prostate_arr[slice,:,:]\n",
    "    image_to_disp_big= adc_arr[0,:,:]\n",
    "    image_to_disp_on_x= adc_arr[-1,:,:]\n",
    "    # mean= np.mean(image_to_disp_big.flatten())\n",
    "    # std= np.std(image_to_disp_big.flatten())\n",
    "    # print(f\"stddd {std}\")\n",
    "    # # image_to_disp_big= np.clip(image_to_disp_big,mean+1000, mean-1000)\n",
    "    # image_to_disp_big= np.clip(image_to_disp_big,-10000.0, 100000.0)\n",
    "\n",
    "    # with_boundaries=mark_boundaries(image_to_disp_big, prost.astype(int) )\n",
    "    # with_boundaries=mark_boundaries(image_to_disp_big, prost.astype(int) )\n",
    "    fig, axs = plt.subplots(ncols=3)\n",
    "\n",
    "    # sns.heatmap(image_to_disp_big,cmap=\"Greys\" ,ax=axs[0])\n",
    "    # sns.heatmap(image_to_disp_on_x,cmap=\"Greys\" ,ax=axs[1])\n",
    "    # plt.imshow(with_boundaries)\n",
    "    sns.heatmap(adc_arr[slice,:,:],cmap=\"Greys\" ,ax=axs[0] )\n",
    "    sns.heatmap(hbv_arr[slice,:,:],cmap=\"Greys\" ,ax=axs[1] )\n",
    "    sns.heatmap(labRes[slice,:,:],cmap=\"Greys\" ,ax=axs[2] )\n",
    "    plt.show()\n",
    "\n",
    "    # registered_modalities= list(map(lambda mod: reg_a_to_b(join(temp_dir,mod),group[0],group[1][main_modality][0],group[1][mod][0],group[1][mod][1],reg_prop\n",
    "    #                                                             ,elacticPath,transformix_path,mod)\n",
    "                        # ,modalities_of_intrest_without_main   ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no prostate! 232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "** ERROR (nifti_image_write_engine): cannot open output file '/workspaces/konwersjaJsonData/explore/temp/234_lesion1_hbv_U_8ZmM76.nii.gz'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Exception thrown in SimpleITK ImageFileWriter_Execute: /tmp/SimpleITK-build/ITK/Modules/IO/NIFTI/src/itkNiftiImageIO.cxx:2254:\nITK ERROR: NiftiImageIO(0x557293ad6160): ERROR: nifti library failed to write image/workspaces/konwersjaJsonData/explore/temp/234_lesion1_hbv_U_8ZmM76.nii.gz",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_216029/3094425460.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#         return pool.map(fun,iterable)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m ids=toolz.pipe(sourceFrame.iterrows()\n\u001b[0m\u001b[1;32m     19\u001b[0m                                 \u001b[0;34m,\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'series_desc'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodalities_of_intrest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                 \u001b[0;34m,\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# filter out all of the test cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/toolz/functoolz.py\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(data, *funcs)\u001b[0m\n\u001b[1;32m    628\u001b[0m     \"\"\"\n\u001b[1;32m    629\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_216029/1635159062.py\u001b[0m in \u001b[0;36mvisualize_range\u001b[0;34m(group, main_modality, modalities_of_intrest, non_mri_inputs, out_folder)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     labels_hbv=list(map(lambda pathh_moving: reg_a_to_b_by_metadata_single_b(sources_dict[main_modality][0],pathh_moving,out_folder,sitk.sitkNearestNeighbor)                                    \n\u001b[0m\u001b[1;32m    137\u001b[0m                                                       ,labels_hbv ))\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_216029/1635159062.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(pathh_moving)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     labels_hbv=list(map(lambda pathh_moving: reg_a_to_b_by_metadata_single_b(sources_dict[main_modality][0],pathh_moving,out_folder,sitk.sitkNearestNeighbor)                                    \n\u001b[0m\u001b[1;32m    137\u001b[0m                                                       ,labels_hbv ))\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_216029/3590748230.py\u001b[0m in \u001b[0;36mreg_a_to_b_by_metadata_single_b\u001b[0;34m(fixed_image_path, moving_image_path, out_folder, interpolator)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mnew_path\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmoving_image_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetFileName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresampled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/SimpleITK/SimpleITK.py\u001b[0m in \u001b[0;36mExecute\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   7911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7912\u001b[0m         \"\"\"\n\u001b[0;32m-> 7913\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_SimpleITK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFileWriter_Execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7915\u001b[0m \u001b[0;31m# Register ImageFileWriter in _SimpleITK:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception thrown in SimpleITK ImageFileWriter_Execute: /tmp/SimpleITK-build/ITK/Modules/IO/NIFTI/src/itkNiftiImageIO.cxx:2254:\nITK ERROR: NiftiImageIO(0x557293ad6160): ERROR: nifti library failed to write image/workspaces/konwersjaJsonData/explore/temp/234_lesion1_hbv_U_8ZmM76.nii.gz"
     ]
    }
   ],
   "source": [
    "\n",
    "test_ids = pd.read_csv(\"/workspaces/konwersjaJsonData/test_ids.csv\").to_numpy().flatten()\n",
    "\n",
    "test_ids= list(map(lambda el: str(el).strip(),test_ids ))\n",
    "filter_ids=lambda row: str(row[1]['masterolds']).strip() not in test_ids\n",
    "modalities_of_intrest=['t2w','adc','hbv']\n",
    "cols=sourceFrame.columns\n",
    "noSegCols=list(filter(lambda el: '_noSeg' in el , cols))+['series_MRI_path']\n",
    "lesion_cols=list(filter(lambda el: 'lesion' in el , noSegCols))\n",
    "main_modality = 'adc'\n",
    "non_mri_inputs=[new_col_name]\n",
    "out_folder='/workspaces/konwersjaJsonData/explore/temp'\n",
    "# with mp.Pool(processes = mp.cpu_count()) as pool:\n",
    "# with mp.Pool(processes = 1) as pool:\n",
    "#     @curry  \n",
    "#     def pmap(fun,iterable):\n",
    "#         return pool.map(fun,iterable)\n",
    "\n",
    "ids=toolz.pipe(sourceFrame.iterrows()\n",
    "                                ,filter(lambda row: row[1]['series_desc'] in modalities_of_intrest)\n",
    "                                ,filter(filter_ids) # filter out all of the test cases\n",
    "                                ,groupByMaster\n",
    "                                ,map(partial(iterGroupModalities,modalities_of_intrest=modalities_of_intrest,label_cols=lesion_cols,non_mri_inputs=non_mri_inputs))\n",
    "                                ,filter(lambda group: ' ' not in group[1].keys() )\n",
    "                                ,list\n",
    "                                ,map(partial(visualize_range,main_modality=main_modality,modalities_of_intrest=modalities_of_intrest,non_mri_inputs=non_mri_inputs,out_folder=out_folder))                            \n",
    "                                ,list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa=sourceFrame['masterolds'].to_numpy()\n",
    "99461 in aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(filter(lambda idd: str(idd) in list(map(str,test_ids)), ids))\n",
    "list(filter(lambda idd: int(idd) in list(map(int,test_ids)) , ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/MIC-DKFZ/nnUNet.git\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
