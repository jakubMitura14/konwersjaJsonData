attention: scaled_dot_product
attn_pdrop: 0.0
betas: !!python/tuple
- 0.9
- 0.99
classifier: TOKEN
dim: 384
hidden_layer_multiplier: 4
image_size: 32
learning_rate: 0.0005
linear_warmup_ratio: 0.1
mlp_pdrop: 0.0
n_head: 6
n_layer: 6
num_classes: 10
patch_size: 2
resid_pdrop: 0.0
residual_norm_style: pre
steps: 78
use_rotary_embeddings: true
weight_decay: 0.03
